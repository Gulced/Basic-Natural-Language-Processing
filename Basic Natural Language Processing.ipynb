{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 72,
   "id": "1d090561-836a-4ac9-8db0-79be3e9e0292",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: NLTK in /opt/anaconda3/lib/python3.12/site-packages (3.8.1)\n",
      "Requirement already satisfied: click in /opt/anaconda3/lib/python3.12/site-packages (from NLTK) (8.1.7)\n",
      "Requirement already satisfied: joblib in /opt/anaconda3/lib/python3.12/site-packages (from NLTK) (1.4.2)\n",
      "Requirement already satisfied: regex>=2021.8.3 in /opt/anaconda3/lib/python3.12/site-packages (from NLTK) (2023.10.3)\n",
      "Requirement already satisfied: tqdm in /opt/anaconda3/lib/python3.12/site-packages (from NLTK) (4.66.4)\n"
     ]
    }
   ],
   "source": [
    "!pip install NLTK"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "f3ecbae7-6bea-464d-b8d1-49905bec7465",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "showing info https://raw.githubusercontent.com/nltk/nltk_data/gh-pages/index.xml\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#NLTK is the natrual language toolkit which help us to extract meaningful information from a given text data\n",
    "import nltk \n",
    "nltk.download()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "id": "d735106c-6a39-442a-a766-3bf733cf0d4b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: gensim in /opt/anaconda3/lib/python3.12/site-packages (4.3.2)\n",
      "Requirement already satisfied: numpy>=1.18.5 in /opt/anaconda3/lib/python3.12/site-packages (from gensim) (1.26.4)\n",
      "Requirement already satisfied: scipy>=1.7.0 in /opt/anaconda3/lib/python3.12/site-packages (from gensim) (1.13.1)\n",
      "Requirement already satisfied: smart-open>=1.8.1 in /opt/anaconda3/lib/python3.12/site-packages (from gensim) (5.2.1)\n"
     ]
    }
   ],
   "source": [
    "!pip install gensim"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "6656198b-df93-4f60-9738-3e4d487e5a41",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting pattern\n",
      "  Downloading Pattern-3.6.0.tar.gz (22.2 MB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m22.2/22.2 MB\u001b[0m \u001b[31m1.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25ldone\n",
      "\u001b[?25hCollecting future (from pattern)\n",
      "  Downloading future-1.0.0-py3-none-any.whl.metadata (4.0 kB)\n",
      "Collecting backports.csv (from pattern)\n",
      "  Downloading backports.csv-1.0.7-py2.py3-none-any.whl.metadata (4.0 kB)\n",
      "Collecting mysqlclient (from pattern)\n",
      "  Downloading mysqlclient-2.2.4.tar.gz (90 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m90.4/90.4 kB\u001b[0m \u001b[31m1.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0ma \u001b[36m0:00:01\u001b[0m\n",
      "\u001b[?25h  Installing build dependencies ... \u001b[?25ldone\n",
      "\u001b[?25h  Getting requirements to build wheel ... \u001b[?25lerror\n",
      "  \u001b[1;31merror\u001b[0m: \u001b[1msubprocess-exited-with-error\u001b[0m\n",
      "  \n",
      "  \u001b[31m×\u001b[0m \u001b[32mGetting requirements to build wheel\u001b[0m did not run successfully.\n",
      "  \u001b[31m│\u001b[0m exit code: \u001b[1;36m1\u001b[0m\n",
      "  \u001b[31m╰─>\u001b[0m \u001b[31m[27 lines of output]\u001b[0m\n",
      "  \u001b[31m   \u001b[0m Trying pkg-config --exists mysqlclient\n",
      "  \u001b[31m   \u001b[0m Command 'pkg-config --exists mysqlclient' returned non-zero exit status 1.\n",
      "  \u001b[31m   \u001b[0m Trying pkg-config --exists mariadb\n",
      "  \u001b[31m   \u001b[0m Command 'pkg-config --exists mariadb' returned non-zero exit status 1.\n",
      "  \u001b[31m   \u001b[0m Trying pkg-config --exists libmariadb\n",
      "  \u001b[31m   \u001b[0m Command 'pkg-config --exists libmariadb' returned non-zero exit status 1.\n",
      "  \u001b[31m   \u001b[0m Traceback (most recent call last):\n",
      "  \u001b[31m   \u001b[0m   File \"/opt/anaconda3/lib/python3.12/site-packages/pip/_vendor/pyproject_hooks/_in_process/_in_process.py\", line 353, in <module>\n",
      "  \u001b[31m   \u001b[0m     main()\n",
      "  \u001b[31m   \u001b[0m   File \"/opt/anaconda3/lib/python3.12/site-packages/pip/_vendor/pyproject_hooks/_in_process/_in_process.py\", line 335, in main\n",
      "  \u001b[31m   \u001b[0m     json_out['return_val'] = hook(**hook_input['kwargs'])\n",
      "  \u001b[31m   \u001b[0m                              ^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  \u001b[31m   \u001b[0m   File \"/opt/anaconda3/lib/python3.12/site-packages/pip/_vendor/pyproject_hooks/_in_process/_in_process.py\", line 118, in get_requires_for_build_wheel\n",
      "  \u001b[31m   \u001b[0m     return hook(config_settings)\n",
      "  \u001b[31m   \u001b[0m            ^^^^^^^^^^^^^^^^^^^^^\n",
      "  \u001b[31m   \u001b[0m   File \"/private/var/folders/cg/31b33_bn1532c6tyzbz_w4_00000gn/T/pip-build-env-3s3uwji2/overlay/lib/python3.12/site-packages/setuptools/build_meta.py\", line 332, in get_requires_for_build_wheel\n",
      "  \u001b[31m   \u001b[0m     return self._get_build_requires(config_settings, requirements=[])\n",
      "  \u001b[31m   \u001b[0m            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  \u001b[31m   \u001b[0m   File \"/private/var/folders/cg/31b33_bn1532c6tyzbz_w4_00000gn/T/pip-build-env-3s3uwji2/overlay/lib/python3.12/site-packages/setuptools/build_meta.py\", line 302, in _get_build_requires\n",
      "  \u001b[31m   \u001b[0m     self.run_setup()\n",
      "  \u001b[31m   \u001b[0m   File \"/private/var/folders/cg/31b33_bn1532c6tyzbz_w4_00000gn/T/pip-build-env-3s3uwji2/overlay/lib/python3.12/site-packages/setuptools/build_meta.py\", line 318, in run_setup\n",
      "  \u001b[31m   \u001b[0m     exec(code, locals())\n",
      "  \u001b[31m   \u001b[0m   File \"<string>\", line 155, in <module>\n",
      "  \u001b[31m   \u001b[0m   File \"<string>\", line 49, in get_config_posix\n",
      "  \u001b[31m   \u001b[0m   File \"<string>\", line 28, in find_package_name\n",
      "  \u001b[31m   \u001b[0m Exception: Can not find valid pkg-config name.\n",
      "  \u001b[31m   \u001b[0m Specify MYSQLCLIENT_CFLAGS and MYSQLCLIENT_LDFLAGS env vars manually\n",
      "  \u001b[31m   \u001b[0m \u001b[31m[end of output]\u001b[0m\n",
      "  \n",
      "  \u001b[1;35mnote\u001b[0m: This error originates from a subprocess, and is likely not a problem with pip.\n",
      "\u001b[1;31merror\u001b[0m: \u001b[1msubprocess-exited-with-error\u001b[0m\n",
      "\n",
      "\u001b[31m×\u001b[0m \u001b[32mGetting requirements to build wheel\u001b[0m did not run successfully.\n",
      "\u001b[31m│\u001b[0m exit code: \u001b[1;36m1\u001b[0m\n",
      "\u001b[31m╰─>\u001b[0m See above for output.\n",
      "\n",
      "\u001b[1;35mnote\u001b[0m: This error originates from a subprocess, and is likely not a problem with pip.\n",
      "\u001b[?25h"
     ]
    }
   ],
   "source": [
    "!pip install pattern"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "695b0bf5-51ef-4854-8f20-d409e7c7541f",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "35fe5a82-ed72-43ae-9531-5f075dae6880",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fd3aebb3-3636-48b5-b5c3-49dfed98004f",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "05caddc3-6976-4719-b5bf-07ad19d81d84",
   "metadata": {},
   "outputs": [],
   "source": [
    "**Tokenizing text data**\n",
    "\n",
    "When we deal with text, we ned to break it down into smaller pieces for analysis. \n",
    "To do this tokenization can be applied.\n",
    "Tokenization is the process of dividing text into a set of pieces such as words or sentences.\n",
    "These pieces are called tokens. Depending on the applications or projects, our methods can divide text into many tokens."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "8e25d76f-6aed-461d-904e-895eaadef783",
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.tokenize import sent_tokenize, word_tokenize,WordPunctTokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "75058823-d981-444b-abf5-cbb10016a6a6",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Define  the input text\n",
    "input_text = \"Do you know how tokenization works? It's actually quite interesting! Let's analyze a couple of sentences and figure it out.\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "872d4c23-c5fa-4e97-bc93-4245cf878041",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      " Sentence tokenizer:  ['Do you know how tokenization works?', \"It's actually quite interesting!\", \"Let's analyze a couple of sentences and figure it out.\"]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to /Users/gulcekoc/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "nltk.download('punkt')\n",
    "# Let's divide the input text into sentence tokens\n",
    "print(\"\\n Sentence tokenizer: \", sent_tokenize(input_text))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "c04350c6-78cb-4baf-8483-08898e8c8a1d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      " Word tokenizer:  ['Do', 'you', 'know', 'how', 'tokenization', 'works', '?', 'It', \"'s\", 'actually', 'quite', 'interesting', '!', 'Let', \"'s\", 'analyze', 'a', 'couple', 'of', 'sentences', 'and', 'figure', 'it', 'out', '.']\n"
     ]
    }
   ],
   "source": [
    "# Let's divide the input text into word tokens\n",
    "print(\"\\n Word tokenizer: \", word_tokenize(input_text))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "bdac6c1d-f7da-4e61-8f1e-54ca5bd23201",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "  Word punct tokenizer:  ['Do', 'you', 'know', 'how', 'tokenization', 'works', '?', 'It', \"'\", 's', 'actually', 'quite', 'interesting', '!', 'Let', \"'\", 's', 'analyze', 'a', 'couple', 'of', 'sentences', 'and', 'figure', 'it', 'out', '.']\n"
     ]
    }
   ],
   "source": [
    "# It is a time to divide the input text into word tokens using WordPunct\n",
    "print(\"\\n  Word punct tokenizer: \",WordPunctTokenizer().tokenize(input_text))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "eea0ef85-eafc-425b-8b84-e4e491bfbf41",
   "metadata": {},
   "outputs": [],
   "source": [
    "#As can be seen from the above result, the input sentence is divided by each word tokens. The word 'its' in the above result, is divided by the punct tokenizer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "01622291-fde1-4537-a535-714ce4be94bd",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Introduction to Stemming and Lemitization\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "2887c152-b51d-4e96-b822-8905f78543ee",
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.stem.porter import PorterStemmer\n",
    "from nltk.stem.lancaster import LancasterStemmer\n",
    "from nltk.stem.snowball import SnowballStemmer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "85ba7416-85ae-4308-88ec-93b25785dbd4",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Define some input words\n",
    "input_words = ['writing', 'calves', 'be', 'branded', 'house', 'randomize', \n",
    "        'possibly', 'extraction', 'hospital', 'kept', 'scratchy', 'code']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "5c91b1f7-048e-4821-b20f-4c0f0da2ccb9",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Create objects for the stemmers\n",
    "porter = PorterStemmer()\n",
    "lancaster = LancasterStemmer()\n",
    "snowball = SnowballStemmer('english')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "7ac5f7c4-495f-4093-9843-a29e2ca81886",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "       INPUT WORD          PORTER       LANCASTER        SNOWBALL \n",
      " ====================================================================\n",
      "         running             run             run             run\n",
      "           jumps            jump            jump            jump\n",
      "          easily          easili            easy          easili\n",
      "          fairly          fairli            fair            fair\n"
     ]
    }
   ],
   "source": [
    "# Create a list of stemmer names for display\n",
    "stemmer_names = ['PORTER', 'LANCASTER', 'SNOWBALL']\n",
    "formatted_text = '{:>16}' * (len(stemmer_names) + 1)\n",
    "print('\\n', formatted_text.format('INPUT WORD', *stemmer_names),\n",
    "        '\\n', '='*68)\n",
    "# We need to Iterate through the words and stem them using the 3 stemmers\n",
    "for word in input_words:\n",
    "    output = [word, porter.stem(word), \n",
    "            lancaster.stem(word), snowball.stem(word)]\n",
    "    print(formatted_text.format(*output))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "53b7002a-d320-481d-bc26-7deffad69dd1",
   "metadata": {},
   "outputs": [],
   "source": [
    "#As can be seen from the above result we did perform 3 stemming algorithms and all of them basically try to achive the same purpose.\n",
    "#The only difference is the level of structness which is used to arrive at the base form.\n",
    "\n",
    "#The Porter stemmer is the least struc and Lancaster is the strictest.\n",
    "#Stemmers behave variously when it comes ot words such as possibly.\n",
    "#The stemmed outputs obtained from the Lancaster stemmer are a bit of obfuscated because it reduce the words a lot\n",
    "#. At the same time the algorithm is fast. The best algorithm is the Snowball stemmer because it is a good trade-off between speed and strictness."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "c5eedb39-9d37-483a-acd8-a0eb8ed03a69",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Introduction to lemmatization\n",
    "\n",
    "\n",
    "#Lemmatization is another method of reducing words to their base forms.\n",
    "#Lemmatization is the process of grouping together the different inflected forms of a word so they can be analyzed as a single item. Lemmatization is like stemming but it brings content to the words."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "ff8cab01-e055-401d-a6d3-64476bd2638b",
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.stem import WordNetLemmatizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "id": "bf698b57-9c44-4b21-aa3a-9413be0ac5c8",
   "metadata": {},
   "outputs": [],
   "source": [
    "input_words = ['writing', 'calves', 'be', 'branded', 'house', 'randomize', \n",
    "        'possibly', 'extraction', 'hospital', 'kept', 'scratchy', 'code']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "id": "f9ccf146-6958-483c-9619-598cc04cb8ea",
   "metadata": {},
   "outputs": [],
   "source": [
    "#create lemmatizer object\n",
    "lemmatizer=WordNetLemmatizer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "id": "f6871cef-9d55-4a57-8623-893129046a24",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package wordnet to\n",
      "[nltk_data]     /Users/gulcekoc/nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n",
      "[nltk_data] Downloading package omw-1.4 to\n",
      "[nltk_data]     /Users/gulcekoc/nltk_data...\n",
      "[nltk_data]   Package omw-1.4 is already up-to-date!\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "               INPUT WORD         NOUN LEMMATIZER         VERB LEMMATIZER \n",
      " ===========================================================================\n",
      "                 writing                 writing                   write\n",
      "                  calves                    calf                   calve\n",
      "                      be                      be                      be\n",
      "                 branded                 branded                   brand\n",
      "                   house                   house                   house\n",
      "               randomize               randomize               randomize\n",
      "                possibly                possibly                possibly\n",
      "              extraction              extraction              extraction\n",
      "                hospital                hospital                hospital\n",
      "                    kept                    kept                    keep\n",
      "                scratchy                scratchy                scratchy\n",
      "                    code                    code                    code\n"
     ]
    }
   ],
   "source": [
    "nltk.download('wordnet')\n",
    "nltk.download('omw-1.4')\n",
    "lemmatizer_names = ['NOUN LEMMATIZER', 'VERB LEMMATIZER']\n",
    "formatted_text = '{:>24}' * (len(lemmatizer_names) + 1)\n",
    "print('\\n', formatted_text.format('INPUT WORD', *lemmatizer_names), \n",
    "        '\\n', '='*75)\n",
    "for word in input_words:\n",
    "    output = [word, lemmatizer.lemmatize(word, pos='n'),\n",
    "           lemmatizer.lemmatize(word, pos='v')]\n",
    "    print(formatted_text.format(*output))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "id": "56d42639-c78c-443a-b228-ef2e0c175333",
   "metadata": {},
   "outputs": [],
   "source": [
    "#As can be seen from the above result, it works different than the stemming. If we see the word writing and calves it is very different from Stemming. \n",
    "#The lemmatizer outputs are all meaningful, while the stemmer ouputs can be sometimes not meaningful."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "60730799-6550-41c0-9ff8-8bb034caf92c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Chunking\n",
    "\n",
    "#Text data usually needs to be divided into pieces for further analysis. \n",
    "#This process is known as chunking. This is used frequently in text analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "id": "5452c24f-f399-44be-90aa-099f624287de",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from nltk.corpus import brown"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 127,
   "id": "fad07f0c-f3e8-4459-b5d7-12dc34982581",
   "metadata": {},
   "outputs": [],
   "source": [
    "#There is a need to define a function to divide the input text into chunks. The first parameter will be text and the second parameter will be the number of words in each chunk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 129,
   "id": "ccc36417-ccda-427b-bdcb-f5d87f799240",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package brown to /Users/gulcekoc/nltk_data...\n",
      "[nltk_data]   Package brown is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "nltk.download('brown')\n",
    "\n",
    "# Split the input text into chunks, where each chunk contains N words\n",
    "def chunker(input_data, N):\n",
    "    input_words = input_data.split(' ')\n",
    "    output = []\n",
    "\n",
    "    cur_chunk = []\n",
    "    count = 0\n",
    "    for word in input_words:\n",
    "        cur_chunk.append(word)\n",
    "        count += 1\n",
    "        if count == N:\n",
    "            output.append(' '.join(cur_chunk))\n",
    "            count, cur_chunk = 0, []\n",
    "\n",
    "    if cur_chunk:  # Son kalan parçayı ekleyin, eğer varsa\n",
    "        output.append(' '.join(cur_chunk))\n",
    "\n",
    "    return output\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 131,
   "id": "7f2cb631-16a0-4fe2-9a2e-3ea54cc8bc15",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Number of text chunks = 20 \n",
      "\n",
      "Chunk 1 ==> The Fulton County Grand Jury said Friday an invest\n",
      "Chunk 2 ==> '' . ( 2 ) Fulton legislators `` work with city of\n",
      "Chunk 3 ==> . Construction bonds Meanwhile , it was learned th\n",
      "Chunk 4 ==> , anonymous midnight phone calls and veiled threat\n",
      "Chunk 5 ==> Harris , Bexar , Tarrant and El Paso would be $451\n",
      "Chunk 6 ==> set it for public hearing on Feb. 22 . The proposa\n",
      "Chunk 7 ==> College . He has served as a border patrolman and \n",
      "Chunk 8 ==> of his staff were doing on the address involved co\n",
      "Chunk 9 ==> plan alone would boost the base to $5,000 a year a\n",
      "Chunk 10 ==> nursing homes In the area of `` community health s\n",
      "Chunk 11 ==> of its Angola policy prove harsh , there has been \n",
      "Chunk 12 ==> system which will prevent Laos from being used as \n",
      "Chunk 13 ==> reform in recipient nations . In Laos , the admini\n",
      "Chunk 14 ==> . He is not interested in being named a full-time \n",
      "Chunk 15 ==> said , `` to obtain the views of the general publi\n",
      "Chunk 16 ==> '' . Mr. Reama , far from really being retired , i\n",
      "Chunk 17 ==> making enforcement of minor offenses more effectiv\n",
      "Chunk 18 ==> to tell the people where he stands on the tax issu\n",
      "Chunk 19 ==> '' . Trenton -- William J. Seidel , state fire war\n",
      "Chunk 20 ==> not comment on tax reforms or other issues in whic\n"
     ]
    }
   ],
   "source": [
    "# Read the first 14000 words from the brown corpus\n",
    "input_data = ' '.join(brown.words()[:14000])\n",
    "\n",
    "# Define the number of words in each chunk\n",
    "chunk_size = 700\n",
    "\n",
    "# Divide input text into chunks and display the output\n",
    "chunks = chunker(input_data, chunk_size)\n",
    "print('\\nNumber of text chunks =', len(chunks), '\\n')\n",
    "for i, chunk in enumerate(chunks):\n",
    "    print('Chunk', i + 1, '==>', chunk[:50])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 233,
   "id": "db5ba009-034a-4389-9e1d-c88a5233b1f1",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Introduction to bag of word\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 235,
   "id": "37dd895d-f56d-42f3-934b-5fa441dc9c9d",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from nltk.corpus import brown\n",
    "import nltk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 237,
   "id": "806fb8f7-14f0-4443-83a8-ad47de37387b",
   "metadata": {},
   "outputs": [],
   "source": [
    "input_data = ' '.join(brown.words()[:5500])\n",
    "chunk_size = 800\n",
    "text = chunker(input_data, chunk_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 239,
   "id": "030c2d61-486c-4bd8-9a60-0970294705cf",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert to dict items\n",
    "chunks = []\n",
    "for count, chunk in enumerate(text):\n",
    "    d = {'index': count, 'text': chunk}\n",
    "    chunks.append(d)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 241,
   "id": "b87225a2-e4a9-4f6c-8823-680086b15818",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extract the document term matrix\n",
    "count_vectorizer = CountVectorizer(min_df=7, max_df=20)\n",
    "document_term_matrix = count_vectorizer.fit_transform([chunk['text'] for chunk in chunks])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 243,
   "id": "76197139-e077-4239-a19b-e544f70200e7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'dict'> {'index': 0, 'text': \"The Fulton County Grand Jury said Friday an investigation of Atlanta's recent primary election produced `` no evidence '' that any irregularities took place . The jury further said in term-end presentments that the City Executive Committee , which had over-all charge of the election , `` deserves the praise and thanks of the City of Atlanta '' for the manner in which the election was conducted . The September-October term jury had been charged by Fulton Superior Court Judge Durwood Pye to investigate reports of possible `` irregularities '' in the hard-fought primary which was won by Mayor-nominate Ivan Allen Jr. . `` Only a relative handful of such reports was received '' , the jury said , `` considering the widespread interest in the election , the number of voters and the size of this city '' . The jury said it did find that many of Georgia's registration and election laws `` are outmoded or inadequate and often ambiguous '' . It recommended that Fulton legislators act `` to have these laws studied and revised to the end of modernizing and improving them '' . The grand jury commented on a number of other topics , among them the Atlanta and Fulton County purchasing departments which it said `` are well operated and follow generally accepted practices which inure to the best interest of both governments '' . Merger proposed However , the jury said it believes `` these two offices should be combined to achieve greater efficiency and reduce the cost of administration '' . The City Purchasing Department , the jury said , `` is lacking in experienced clerical personnel as a result of city personnel policies '' . It urged that the city `` take steps to remedy '' this problem . Implementation of Georgia's automobile title law was also recommended by the outgoing jury . It urged that the next Legislature `` provide enabling funds and re-set the effective date so that an orderly implementation of the law may be effected '' . The grand jury took a swipe at the State Welfare Department's handling of federal funds granted for child welfare services in foster homes . `` This is one of the major items in the Fulton County general assistance program '' , the jury said , but the State Welfare Department `` has seen fit to distribute these funds through the welfare departments of all the counties in the state with the exception of Fulton County , which receives none of this money . The jurors said they realize `` a proportionate distribution of these funds might disable this program in our less populous counties '' . Nevertheless , `` we feel that in the future Fulton County should receive some portion of these available funds '' , the jurors said . `` Failure to do this will continue to place a disproportionate burden '' on Fulton taxpayers . The jury also commented on the Fulton ordinary's court which has been under fire for its practices in the appointment of appraisers , guardians and administrators and the awarding of fees and compensation . Wards protected The jury said it found the court `` has incorporated into its operating procedures the recommendations '' of two previous grand juries , the Atlanta Bar Association and an interim citizens committee . `` These actions should serve to protect in fact and in effect the court's wards from undue costs and its appointed and elected servants from unmeritorious criticisms '' , the jury said . Regarding Atlanta's new multi-million-dollar airport , the jury recommended `` that when the new management takes charge Jan. 1 the airport be operated in a manner that will eliminate political influences '' . The jury did not elaborate , but it added that `` there should be periodic surveillance of the pricing practices of the concessionaires for the purpose of keeping the prices reasonable '' . Ask jail deputies On other matters , the jury recommended that : ( 1 ) Four additional deputies be employed at the Fulton County Jail and `` a doctor , medical intern or extern be employed for night and weekend duty at the jail '' . ( 2 ) Fulton legislators `` work with city officials to pass enabling legislation that will permit the establishment of a fair and equitable '' pension plan for city employes . The jury praised the administration and operation of the Atlanta Police Department , the Fulton Tax Commissioner's Office , the Bellwood and Alpharetta prison farms , Grady Hospital and the Fulton Health Department . Mayor William B. Hartsfield filed suit for divorce from his wife , Pearl Williams Hartsfield , in Fulton Superior Court Friday . His petition charged mental cruelty . The couple was married Aug.\"}\n"
     ]
    }
   ],
   "source": [
    "#İlk öğenin doğru yapıda olduğunu kontrol edelim\n",
    "print(type(chunks[0]), chunks[0])  # Sözlük olup olmadığını kontrol ediyoruz"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 245,
   "id": "201d1b9a-b324-406e-b7be-edd7ad9bfe5d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Document Term Matrix başarıyla oluşturuldu!\n"
     ]
    }
   ],
   "source": [
    "# Belge-terim matrisi oluşturmak için CountVectorizer kullanıyoruz\n",
    "# min_df ve max_df değerlerini uygun şekilde ayarlıyoruz\n",
    "count_vectorizer = CountVectorizer(min_df=7, max_df=20)  # Daha az sınırlayıcı ayarlamalar\n",
    "\n",
    "# chunks listesindeki her 'text' anahtarını kullanarak fit_transform işlemi yapıyoruz\n",
    "try:\n",
    "    document_term_matrix = count_vectorizer.fit_transform([chunk['text'] for chunk in chunks])\n",
    "    print(\"Document Term Matrix başarıyla oluşturuldu!\")\n",
    "except ValueError as e:\n",
    "    print(\"Hata:\", e)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 252,
   "id": "b8c1c085-728e-448f-80d5-47fbb77429ff",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Sonraki adımda, Bag of Words modelini kullanarak kelime dağarcığını (vocabulary) çıkaracağız ve bunu görüntüleyeceğiz. Kelime dağarcığı, çıkarılan ayırt edici kelimelerin listesidir."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 259,
   "id": "052efb4d-fe1e-4bce-badb-b94f8ca092e4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Vocabulary:\n",
      " ['and' 'are' 'be' 'by' 'county' 'for' 'in' 'is' 'it' 'of' 'on' 'one'\n",
      " 'said' 'state' 'that' 'the' 'to' 'two' 'was' 'which' 'with']\n"
     ]
    }
   ],
   "source": [
    " # Özellik adlarını alıyoruz\n",
    "vocabulary = np.array(count_vectorizer.get_feature_names_out())\n",
    "print(\"\\nVocabulary:\\n\", vocabulary)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 261,
   "id": "d294ac57-b727-4c15-9345-2aa3f65296b3",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Generate names for chunks\n",
    "chunk_names=[]\n",
    "for i in range(len(text)):\n",
    "    chunk_names.append('Chunk-'+str(i+1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 263,
   "id": "a952e354-8aed-4bb7-9221-6ff59b2001f7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      " Document term matrix: \n",
      "\n",
      "         Word     Chunk-1     Chunk-2     Chunk-3     Chunk-4     Chunk-5     Chunk-6     Chunk-7 \n",
      "\n",
      "         and          23           9           9          11           9          17          15\n",
      "         are           2           2           1           1           2           2           2\n",
      "          be           6           8           7           7           6           2           2\n",
      "          by           3           4           4           5          14           3           7\n",
      "      county           6           2           7           3           1           2           2\n",
      "         for           7          13           4          10           7           6           5\n",
      "          in          15          11          15          11          13          14          19\n",
      "          is           2           7           3           4           5           5           2\n",
      "          it           8           6           8           9           3           1           2\n",
      "          of          31          20          20          30          29          35          27\n",
      "          on           4           3           5          10           6           5           3\n",
      "         one           1           3           1           2           2           1           1\n",
      "        said          12           5           7           7           4           3           7\n",
      "       state           3           7           2           6           3           4           1\n",
      "        that          13           8           9           2           7           1           7\n",
      "         the          71          51          43          51          43          52          51\n",
      "          to          11          26          20          26          21          15          14\n",
      "         two           2           1           1           1           1           2           2\n",
      "         was           5           6           7           7           4           7           3\n",
      "       which           7           4           5           4           3           1           1\n",
      "        with           2           2           3           1           2           2           4\n"
     ]
    }
   ],
   "source": [
    "print('\\n Document term matrix: ')\n",
    "formatted_text='{:>12}'*(len(chunk_names)+1)\n",
    "print('\\n', formatted_text.format('Word',*chunk_names), '\\n')\n",
    "for word,item in zip(vocabulary,document_term_matrix.T):\n",
    "    #item is a csr_matrix data structure\n",
    "    output=[word]+[str(freq) for freq in item.data]\n",
    "    print(formatted_text.format(*output))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9e690b18-618b-4af8-bcc2-8846eeb731b7",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
